{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, label_binarize\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from numba import njit, prange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(fastmath=True, parallel=True)\n",
    "def _compute_shapley_values(X_train, y_train, X_test, y_test, K=1):\n",
    "    N = len(X_train)\n",
    "    M = len(X_test)\n",
    "    result = np.zeros(N, dtype=np.float32)\n",
    "\n",
    "    for j in prange(M):\n",
    "        score = np.zeros(N, dtype=np.float32)\n",
    "        dist = np.zeros(N, dtype=np.float32)\n",
    "        div_range = np.arange(1.0, N)\n",
    "        div_min = np.minimum(div_range, K)\n",
    "        for i in range(N):\n",
    "            dist[i] = np.sqrt(np.sum(np.square(X_train[i] - X_test[j])))\n",
    "        indices = np.argsort(dist)\n",
    "        y_sorted = y_train[indices]\n",
    "        eq_check = (y_sorted == y_test[j]) * 1.0\n",
    "        diff = - 1 / K * (eq_check[1:] - eq_check[:-1])\n",
    "        diff /= div_range\n",
    "        diff *= div_min\n",
    "        score[indices[:-1]] = diff\n",
    "        score[indices[-1]] = eq_check[-1] / N\n",
    "        score[indices] += np.sum(score[indices]) - np.cumsum(score[indices])\n",
    "        result += score / M\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(img_str):\n",
    "    return np.array([int(val) for val in img_str.split(':')])\n",
    "\n",
    "def normalise_image(images):\n",
    "    return images / 255.0\n",
    "\n",
    "\n",
    "def reshape_images(images):\n",
    "    return np.concatenate(images['image'].values) \\\n",
    "        .reshape(images.shape[0], 28, 28, 1)\n",
    "\n",
    "\n",
    "def create_cnn():\n",
    "    model = Sequential([\n",
    "        Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "        MaxPooling2D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'),\n",
    "        MaxPooling2D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(f'arguseyes/example_pipelines/datasets/sneakers/product_images.csv', converters={'image': decode_image})\n",
    "\n",
    "product_categories = pd.read_csv('arguseyes/example_pipelines/datasets/sneakers/product_categories.csv')\n",
    "with_categories = train_data.merge(product_categories, on='category_id')\n",
    "\n",
    "categories_to_distinguish = ['Sneaker', 'Ankle boot']\n",
    "\n",
    "images_of_interest = with_categories[with_categories['category_name'].isin(categories_to_distinguish)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('normalisation', FunctionTransformer(normalise_image)),\n",
    "    ('reshaping', FunctionTransformer(reshape_images)),\n",
    "    ('model', KerasClassifier(create_cnn, epochs=10, verbose=0))\n",
    "])\n",
    "\n",
    "random_seed_for_splitting = 1337\n",
    "\n",
    "train, test = train_test_split(images_of_interest, test_size=0.2, random_state=random_seed_for_splitting)\n",
    "\n",
    "y_train = label_binarize(train['category_name'], classes=categories_to_distinguish)\n",
    "y_test = label_binarize(test['category_name'], classes=categories_to_distinguish)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dk/pbhh8rgn191dtpwnbsn_q1mm0000gn/T/ipykernel_19104/2929770763.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['id'] = np.array(range(len(train)))\n"
     ]
    }
   ],
   "source": [
    "train['id'] = np.array(range(len(train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp(seed):\n",
    "    np.random.seed(seed)\n",
    "    y_train_dirty = np.copy(y_train)\n",
    "    mislabeled_idx = np.random.choice(range(len(y_train_dirty)), replace=False, size=475)\n",
    "    y_train_dirty[mislabeled_idx] = np.logical_not(y_train_dirty[mislabeled_idx])\n",
    "    mislabeled_identifiers = set(train.iloc[mislabeled_idx]['id'])\n",
    "\n",
    "    train_copy = train.copy(deep=True)\n",
    "    test_copy = test.copy(deep=True)\n",
    "    y_train_dirty_copy = np.copy(y_train_dirty)\n",
    "\n",
    "    model = pipeline.fit(train_copy[['image']], y_train_dirty_copy)\n",
    "    print('Initial acc: ', model.score(test_copy[['image']], y_test))\n",
    "\n",
    "    for round in range(10):\n",
    "        print('ROUND', round)\n",
    "\n",
    "        X_train = model.steps[1][1].transform(model.steps[0][1].transform(train_copy[['image']]))\n",
    "        X_test = model.steps[1][1].transform(model.steps[0][1].transform(test_copy[['image']]))\n",
    "\n",
    "        s = 100 \n",
    "        k = 10\n",
    "        step_size = 50\n",
    "\n",
    "        X_test_sampled = X_test[:s, :]\n",
    "        y_test_sampled = y_test[:s, :]\n",
    "\n",
    "        shapley_values = _compute_shapley_values(X_train, \n",
    "                                                 np.squeeze(y_train_dirty_copy),         \n",
    "                                                 X_test_sampled,\n",
    "                                                 np.squeeze(y_test_sampled), \n",
    "                                                 k)\n",
    "\n",
    "        redundant_idx = np.argsort(shapley_values)[:step_size]\n",
    "        chosen_identifiers = set(train_copy.iloc[redundant_idx]['id'])\n",
    "\n",
    "        idx_to_keep = np.array([pos for pos in range(len(y_train_dirty_copy)) if pos not in redundant_idx])\n",
    "        train_copy = train_copy.iloc[idx_to_keep]\n",
    "        y_train_dirty_copy = y_train_dirty_copy[idx_to_keep]\n",
    "\n",
    "        print('# Correctly identified', len(chosen_identifiers & mislabeled_identifiers))\n",
    "        print('# samples', len(train_copy))\n",
    "\n",
    "        model = pipeline.fit(train_copy[['image']], y_train_dirty_copy)\n",
    "\n",
    "        print('Acc: ', model.score(test_copy[['image']], y_test))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cleanlab\n",
      "  Downloading cleanlab-2.0.0-py2.py3-none-any.whl (95 kB)\n",
      "\u001b[K     |████████████████████████████████| 95 kB 2.3 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in ./venv/lib/python3.9/site-packages (from cleanlab) (1.7.0)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in ./venv/lib/python3.9/site-packages (from cleanlab) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.11.3 in ./venv/lib/python3.9/site-packages (from cleanlab) (1.19.5)\n",
      "Collecting tqdm>=4.53.0\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 10.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.0.0 in ./venv/lib/python3.9/site-packages (from cleanlab) (1.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in ./venv/lib/python3.9/site-packages (from pandas>=1.0.0->cleanlab) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in ./venv/lib/python3.9/site-packages (from pandas>=1.0.0->cleanlab) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas>=1.0.0->cleanlab) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv/lib/python3.9/site-packages (from scikit-learn>=0.18->cleanlab) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in ./venv/lib/python3.9/site-packages (from scikit-learn>=0.18->cleanlab) (1.0.1)\n",
      "Installing collected packages: tqdm, cleanlab\n",
      "Successfully installed cleanlab-2.0.0 tqdm-4.64.0\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/Users/ssc/projects/arguseyes/venv/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install cleanlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp_noshapley(seed):\n",
    "    np.random.seed(seed)\n",
    "    y_train_dirty = np.copy(y_train)\n",
    "    mislabeled_idx = np.random.choice(range(len(y_train_dirty)), replace=False, size=475)\n",
    "    y_train_dirty[mislabeled_idx] = np.logical_not(y_train_dirty[mislabeled_idx])\n",
    "    mislabeled_identifiers = set(train.iloc[mislabeled_idx]['id'])\n",
    "\n",
    "    train_copy = train.copy(deep=True)\n",
    "    test_copy = test.copy(deep=True)\n",
    "    y_train_dirty_copy = np.copy(y_train_dirty)\n",
    "\n",
    "    model = pipeline.fit(train_copy[['image']], y_train_dirty_copy)\n",
    "    print('Initial acc: ', model.score(test_copy[['image']], y_test))\n",
    "\n",
    "    \n",
    "    X_train = model.steps[1][1].transform(model.steps[0][1].transform(train_copy[['image']]))\n",
    "    X_test = model.steps[1][1].transform(model.steps[0][1].transform(test_copy[['image']]))\n",
    "    \n",
    "    #issues = CleanLearning(LogisticRegression, seed=seed).find_label_issues(data, labels)\n",
    "    \n",
    "    step_size=50\n",
    "    \n",
    "    for round in range(10):\n",
    "        print('ROUND', round)\n",
    "    \n",
    "        probs = model.predict_proba(train_copy[['image']])\n",
    "        diffs = abs(probs[:,0] - probs[:,1])\n",
    "        redundant_idx = np.argsort(diffs)[:step_size]\n",
    "        \n",
    "        redundant_idx = np.random.choice(range(len(train_copy)), replace=False, size=step_size)\n",
    "        chosen_identifiers = set(train_copy.iloc[redundant_idx]['id'])\n",
    "\n",
    "        idx_to_keep = np.array([pos for pos in range(len(y_train_dirty_copy)) if pos not in redundant_idx])\n",
    "        train_copy = train_copy.iloc[idx_to_keep]\n",
    "        y_train_dirty_copy = y_train_dirty_copy[idx_to_keep]\n",
    "\n",
    "        print('# Correctly identified', len(chosen_identifiers & mislabeled_identifiers))\n",
    "        print('# samples', len(train_copy))\n",
    "\n",
    "        model = pipeline.fit(train_copy[['image']], y_train_dirty_copy)\n",
    "\n",
    "        print('Acc: ', model.score(test_copy[['image']], y_test))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial acc:  0.5252100825309753\n",
      "ROUND 0\n",
      "# Correctly identified 30\n",
      "# samples 900\n",
      "Acc:  0.5714285969734192\n",
      "ROUND 1\n",
      "# Correctly identified 27\n",
      "# samples 850\n",
      "Acc:  0.5630252361297607\n",
      "ROUND 2\n",
      "# Correctly identified 25\n",
      "# samples 800\n",
      "Acc:  0.5714285969734192\n",
      "ROUND 3\n",
      "# Correctly identified 29\n",
      "# samples 750\n",
      "Acc:  0.529411792755127\n",
      "ROUND 4\n",
      "# Correctly identified 25\n",
      "# samples 700\n",
      "Acc:  0.5042017102241516\n",
      "ROUND 5\n",
      "# Correctly identified 24\n",
      "# samples 650\n",
      "Acc:  0.7478991746902466\n",
      "ROUND 6\n",
      "# Correctly identified 29\n",
      "# samples 600\n",
      "Acc:  0.7058823704719543\n",
      "ROUND 7\n",
      "# Correctly identified 25\n",
      "# samples 550\n",
      "Acc:  0.5462185144424438\n",
      "ROUND 8\n",
      "# Correctly identified 20\n",
      "# samples 500\n",
      "Acc:  0.5042017102241516\n",
      "ROUND 9\n",
      "# Correctly identified 26\n",
      "# samples 450\n",
      "Acc:  0.7184873819351196\n"
     ]
    }
   ],
   "source": [
    "run_exp_noshapley(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
